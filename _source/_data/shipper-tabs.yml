# This YAML feeds the "ship your data" page.
# There are 2 root-level objects: `tabs` and `tags`

# Tabs is an array that contains the collections to be included in the shipping page.
# Each array item is a single entry like this - `collection: <collection-name>`
# The collection name has to match a collection that's declared in Jekyll's _config.yml (in the `collections` object)
# This will support future data types — like traces — when we're ready to make a collection for it.
# So when you're ready to add a collection in the future, follow these steps:
#   1. Add the collection to the config yaml
#   2. Make a folder with the same name in the logzio_collections/ folder
#   3. Add that collection here, in the `tabs` object.
# Keep in mind there are other collections as well, so not all collections are included in this file.

# The community-shippers collection links out to shippers and projects that we don't maintain.

tabs:
  - collection: log-sources
    templated: true
  - collection: prometheus-sources
    templated: true
    top-content:
  - collection: tracing-sources
    templated: true
  - collection: security-sources
    templated: true
  - collection: community-shippers
    top-content: >-
      <p class="info-box important no-title">
        <span class="bold">Logz.io does not test or support these projects.</span><br />
        These projects are maintained by third parties and recommended by members of the Logz.io community.
        Always test and review the code of any community project before using it.
      </p>
  - collection: metrics-sources
    templated: true
    top-content: >-
      <p class="info-box important no-title">
        <span class="bold"> These are the Metricbeat integrations.<br> If you started using Logz.io metrics in March 2021 or later, select the relevant data shipping option from the <a href="https://app.logz.io/#/dashboard/send-your-data?tag=all&collection=prometheus-sources"> Metrics </a> tab.
        <br> </br>
        Contact your Logz.io Customer Success Manager for assistance with selecting the best Metrics data shipping options for your environments. 
      </p>

# Tags is an array of tags that can be filtered on the data shipping page.
# Each item in the array contains two properties:
#   `slug: <kebab-cased-machine-readable-tag>` (lowercase, dashes only, no other characters)
#   `name: <human readable name for this tag>` (This will be displayed in the docs)
# Tags are used as an ID, so it's best to avoid changing slugs. Users will never see the slug.
# If you want to change the way it's displayed in the docs, change the `name`.

tags:
  - slug: popular
    name: Popular
    order: 1
  - slug: aws
    name: AWS
    order: 2
  - slug: agents
    name: Agents
    order: 2
  - slug: azure
    name: Azure
    order: 3 
  - slug: ci-cd
    name: CI/CD
    order: 4
  - slug: components
    name: Tracing components
    order: 5
  - slug: container
    name: Containers & pods
    order: 6
  - slug: custom-metrics
    name: Custom metrics
    order: 7
  - slug: database
    name: Databases
    order: 7
  - slug: endpoint-security
    name: Endpoint security
    order: 8
  - slug: existing-instrumentation
    name: My code is instrumented
    order: 9
  - slug: firewalls
    name: Firewalls
    order: 10
  - slug: from-your-code
    name: From your code
    order: 11
  - slug: gcp
    name: Google Cloud
    order: 12
  - slug: ids
    name: IDS
    order: 13
  - slug: identity
    name: Identity providers
    order: 14
  - slug: instrumentation
    name: Tracing instrumentation     
    order: 15
  - slug: iot
    name: IoT
    order: 16
  - slug: k8s
    name: K8S
    order: 17
  - slug: linux
    name: Linux security
    order: 18
  - slug: networking
    name: Networking
    order: 19
  - slug: new-instrumentation
    name: My code is not instrumented   
    order: 20
  - slug: platform-service
    name: Platforms & services
    order: 21
  - slug: prebuilt-dashboards
    name: Pre-built dashboards
    order: 22
  - slug: prometheus
    name: Prometheus as a service
    order: 23
  - slug: os
    name: Operating systems
    order: 24
  - slug: security
    name: Security
    order: 25
  - slug: server-app
    name: Server apps & devices
    order: 26
  - slug: traces
    name: Tracing
    order: 27
  - slug: vulnerability-scanners
    name: Vulnerability scanners
    order: 28
  - slug: web-firewalls
    name: Web application firewalls
    order: 29
  - slug: windows
    name: Windows security
    order: 30




# templates is an array of rough procedures that are used by multiple shipping
# docs.
# Each item in the array contains two properties:
#   slug: <kebab-cased-machine-readable-tag> (lowercase, dashes only, no other characters)
#   outline: |
#     Starts ^ with a pipe
#     Multiline explanation of the template, and the rough procedure each doc
#     follows. The entire block must be indented two spaces from `outline`.
# This is shown on the docs-admin/data-shipping-templates/ page.

templates:
  - slug: azure-deployment-event-hubs
    outline: |
      **Intro section**: More information

      * What am I setting up in my Azure account?
      * How many automated deployments should I... deploy?

      **Procedure**:

      1. Configure an automated deployment
      1. Set blob container permissions
      1. Build an event subscription
      1. Check Logz.io for your logs

  - slug: beats-logs
    outline: |
      **Procedure**:

      1. [Sometimes] Configure <this source> logging output
      1. Download the Logz.io public certificate
      1. Configure <this source> as an input
      1. Set Logz.io as the output
      1. Restart <shipper>
      1. Check Logz.io for your logs

  - slug: cloudformation
    outline: |
      **Procedure**:

      1. [GuardDuty only] Create a new Kinesis data stream
      1. [GuardDuty only] Configure CloudWatch Events
      1. Zip the source files
      1. Create the CloudFormation package and upload to S3
      1. Deploy the CloudFormation package
      1. Set the CloudWatch Logs event trigger
      1. Check Logz.io for your logs

  - slug: docker
    outline: |
      **Procedure**:

      1. [Okta only] Get credentials
      1. Pull the Docker image
      1. Run the Docker image
      1. Check Logz.io for your logs

  - slug: docker-metricbeat
    outline: |
      **Intro note**:

      If you're not already running Docker Metrics Collector,
      follow these steps.

      Otherwise, stop the container, add
      `<module>`
      to the `LOGZIO_MODULES` environment variable,
      [_Docker module only:_ add `-v /var/run/docker.sock:/var/run/docker.sock:ro` to the command,]
      and restart.
      You can find the `run` command and all parameters
      in this procedure.

      The `<module>` module collects these metrics:
      `<metric 1>`,
      `<metric 2>`,
      `<metric 3>`,
      `<metric 4>`

      **Procedure**:

      1. [AWS module only] Set up your IAM user
      1. [AWS module only] Get your metrics region
      1. [EC2 Auto Scaling only] Enable EC2 Auto Scaling metrics
      1. [S3 only] _(Optional)_ Enable S3 request metrics
      1. Pull the Docker image
      1. Run the container
        (separate headings for generic and module-specific parameters)
      1. Check Logz.io for your metrics
        (include the dashboard name)

  - slug: k8s-daemonset
    outline: |
      **Procedure**:

      1. Store your Logz.io credentials
      1. Deploy the DaemonSet
      1. Check Logz.io for your logs

  - slug: lambda-cloudwatch
    outline: |
      **Procedure**:

      1. Create a new Lambda function
      1. [Sometimes] Create a new IAM Role
      1. Zip the source files
      1. Upload the zip file and set environment variables
      1. Configure the function's basic settings
      1. Set the CloudWatch Logs event trigger
      1. Check Logz.io for your logs


  - slug: lambda-kinesis
    outline: |
      **Procedure**:

      1. [GuardDuty] Create a new Kinesis data stream
      1. [GuardDuty] Configure CloudWatch Events
      1. [GuardDuty] Create a new IAM role
      1. Create a new Lambda function
      1. Zip the source files
      1. Upload the zip file and set environment variables
      1. Configure the function's basic settings
      1. Set the Kinesis event trigger
      1. Check Logz.io for your logs

  - slug: network-device-filebeat
    outline: |
      **Procedure**:

      1. Configure <the device>
      1. Download the Logz.io public certificate to your Filebeat server
      1. Add <UDP/TCP> traffic as an input
      1. Set Logz.io as the output
      1. Start Filebeat
      1. Check Logz.io for your logs

  - slug: s3-fetcher
    outline: |
      **Procedure**:

      1. Send your logs to an S3 bucket
      1. Add the S3 bucket information
      1. Check Logz.io for your logs
